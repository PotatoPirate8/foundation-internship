"""
Corrected Training Loop: CNN with Proper Normalization
=====================================================

Since the images DO contain the vital signs as text (generated by matplotlib),
but OCR can't read matplotlib-rendered text reliably, we'll use the improved CNN
approach with the metadata ground truth and train until target performance.
"""

import os
import numpy as np
import cv2
import json
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import joblib
from datetime import datetime
import matplotlib.pyplot as plt

class PersistentCNNTrainer:
    """CNN trainer that continues training until target MAE is achieved"""
    
    def __init__(self):
        self.model = None
        self.image_size = (224, 224)
        self.vital_signs_labels = ['heart_rate', 'systolic_bp', 'diastolic_bp', 'spo2', 'temperature', 'pulse_rate']
        self.target_scaler = StandardScaler()
        self.scaler_fitted = False
        self.training_history = []
        self.best_mae = float('inf')

    def preprocess_image(self, image_path):
        """Advanced image preprocessing for matplotlib-generated images"""
        image = cv2.imread(image_path)
        if image is None:
            return None
        
        # Convert BGR to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Apply CLAHE for better contrast (helps with text regions)
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        lab[:,:,0] = clahe.apply(lab[:,:,0])
        image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        
        # Resize with high-quality interpolation
        image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_AREA)
        
        # Normalize to [0,1]
        image = image.astype(np.float32) / 255.0
        
        # ImageNet normalization for transfer learning
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = (image - mean) / std
        
        return image

    def load_dataset(self, dataset_dir):
        """Load dataset with proper normalization"""
        metadata_path = os.path.join(dataset_dir, "dataset_info.json")
        with open(metadata_path, 'r') as f:
            dataset_info = json.load(f)
        
        images = []
        labels = []
        
        print(f"üì¶ Loading {len(dataset_info)} images...")
        
        for item in dataset_info:
            image_path = os.path.join(dataset_dir, item['filename'])
            if os.path.exists(image_path):
                try:
                    # Preprocess image
                    image = self.preprocess_image(image_path)
                    if image is not None:
                        images.append(image)
                        
                        # Extract vital signs in correct order
                        label_vector = [
                            item['vitals']['heart_rate'],
                            item['vitals']['systolic_bp'],
                            item['vitals']['diastolic_bp'],
                            item['vitals']['spo2'],
                            item['vitals']['temperature'],
                            item['vitals']['pulse_rate']
                        ]
                        labels.append(label_vector)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing {item['filename']}: {e}")
        
        images = np.array(images)
        labels = np.array(labels)
        
        print(f"‚úÖ Loaded {len(images)} images successfully")
        
        # CRITICAL: Normalize target values
        if not self.scaler_fitted:
            self.target_scaler.fit(labels)
            self.scaler_fitted = True
            joblib.dump(self.target_scaler, 'persistent_target_scaler.pkl')
            print("‚úÖ Target scaler fitted and saved")
        
        normalized_labels = self.target_scaler.transform(labels)
        
        return images, normalized_labels, labels

    def create_improved_model(self):
        """Create improved CNN model with transfer learning"""
        print("üèóÔ∏è Creating improved CNN model...")
        
        # Use EfficientNetB0 as backbone
        base_model = keras.applications.EfficientNetB0(
            weights='imagenet',
            include_top=False,
            input_shape=(*self.image_size, 3)
        )
        
        # Freeze early layers, fine-tune later ones
        for layer in base_model.layers[:-30]:
            layer.trainable = False
        
        # Build full model
        inputs = keras.Input(shape=(*self.image_size, 3))
        
        # Preprocessing
        x = keras.applications.efficientnet.preprocess_input(inputs)
        
        # Feature extraction
        x = base_model(x, training=False)
        
        # Global pooling
        x = keras.layers.GlobalAveragePooling2D()(x)
        
        # Dense layers with proper regularization
        x = keras.layers.Dense(1024, activation='relu')(x)
        x = keras.layers.BatchNormalization()(x)
        x = keras.layers.Dropout(0.4)(x)
        
        x = keras.layers.Dense(512, activation='relu')(x)
        x = keras.layers.BatchNormalization()(x)
        x = keras.layers.Dropout(0.3)(x)
        
        x = keras.layers.Dense(256, activation='relu')(x)
        x = keras.layers.BatchNormalization()(x)
        x = keras.layers.Dropout(0.2)(x)
        
        x = keras.layers.Dense(128, activation='relu')(x)
        x = keras.layers.Dropout(0.1)(x)
        
        # Output layer
        outputs = keras.layers.Dense(6, activation='linear')(x)
        
        model = keras.Model(inputs, outputs)
        
        # Compile with appropriate settings
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='huber',  # More robust than MSE
            metrics=['mae']
        )
        
        print("‚úÖ Model created successfully")
        return model

    def train_until_target(self, dataset_dir, target_mae=2.0, max_iterations=50, epochs_per_iteration=25):
        """Train model until target MAE is achieved"""
        
        print("üöÄ PERSISTENT CNN TRAINING")
        print("=" * 60)
        print(f"üéØ Target MAE: {target_mae}")
        print(f"üîÑ Max iterations: {max_iterations}")
        print(f"üìä Epochs per iteration: {epochs_per_iteration}")
        
        # Load dataset
        X, y_norm, y_orig = self.load_dataset(dataset_dir)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_norm, test_size=0.2, random_state=42
        )
        
        print(f"üìà Training samples: {len(X_train)}")
        print(f"üìâ Test samples: {len(X_test)}")
        
        # Training loop
        for iteration in range(max_iterations):
            print(f"\nüîÑ ITERATION {iteration + 1}/{max_iterations}")
            print("-" * 40)
            
            # Create or load model
            if self.model is None:
                self.model = self.create_improved_model()
            
            # Callbacks for this iteration
            callbacks = [
                keras.callbacks.EarlyStopping(
                    monitor='val_mae', 
                    patience=15, 
                    restore_best_weights=True, 
                    verbose=0,
                    min_delta=0.001
                ),
                keras.callbacks.ReduceLROnPlateau(
                    monitor='val_mae', 
                    factor=0.7, 
                    patience=8, 
                    min_lr=1e-8, 
                    verbose=1
                )
            ]
            
            # Train for this iteration
            print(f"üèãÔ∏è Training for {epochs_per_iteration} epochs...")
            
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=epochs_per_iteration,
                batch_size=4,  # Small batch size for better convergence
                callbacks=callbacks,
                verbose=1
            )
            
            # Evaluate current performance
            test_loss, test_mae_norm = self.model.evaluate(X_test, y_test, verbose=0)
            
            # Denormalize to get actual MAE
            y_pred_norm = self.model.predict(X_test, verbose=0)
            y_pred = self.target_scaler.inverse_transform(y_pred_norm)
            y_test_actual = self.target_scaler.inverse_transform(y_test)
            
            actual_mae = mean_absolute_error(y_test_actual, y_pred)
            
            # Store history
            iteration_result = {
                'iteration': iteration + 1,
                'normalized_mae': test_mae_norm,
                'actual_mae': actual_mae,
                'test_loss': test_loss
            }
            self.training_history.append(iteration_result)
            
            print(f"üìä Results:")
            print(f"   Normalized MAE: {test_mae_norm:.4f}")
            print(f"   Actual MAE: {actual_mae:.4f}")
            print(f"   Test Loss: {test_loss:.4f}")
            
            # Check if target achieved
            if actual_mae < target_mae:
                print(f"üéâ TARGET ACHIEVED! MAE {actual_mae:.4f} < {target_mae}")
                
                # Save best model
                self.model.save('persistent_cnn_best.keras')
                print("üíæ Best model saved as 'persistent_cnn_best.keras'")
                
                # Show detailed results
                self.show_detailed_results(X_test, y_test, y_orig[-len(X_test):])
                return True
            
            # Track best MAE
            if actual_mae < self.best_mae:
                self.best_mae = actual_mae
                self.model.save('persistent_cnn_checkpoint.keras')
                print(f"‚úÖ New best MAE: {actual_mae:.4f} (saved checkpoint)")
            
            # Adjust learning rate if stuck
            if iteration > 0 and actual_mae > self.training_history[-2]['actual_mae']:
                current_lr = float(self.model.optimizer.learning_rate)
                new_lr = current_lr * 0.8
                self.model.optimizer.learning_rate.assign(new_lr)
                print(f"üìâ Reduced learning rate to {new_lr:.2e}")
        
        print(f"\n‚ö†Ô∏è Maximum iterations reached. Best MAE: {self.best_mae:.4f}")
        
        # Load best checkpoint
        if os.path.exists('persistent_cnn_checkpoint.keras'):
            self.model = keras.models.load_model('persistent_cnn_checkpoint.keras')
            self.show_detailed_results(X_test, y_test, y_orig[-len(X_test):])
        
        return False

    def show_detailed_results(self, X_test, y_test_norm, y_test_orig):
        """Show detailed evaluation results"""
        
        print(f"\nüìä DETAILED EVALUATION RESULTS")
        print("=" * 50)
        
        # Make predictions
        y_pred_norm = self.model.predict(X_test, verbose=0)
        y_pred = self.target_scaler.inverse_transform(y_pred_norm)
        y_test = self.target_scaler.inverse_transform(y_test_norm)
        
        # Per-vital MAE
        print(f"üìà PER-VITAL SIGN PERFORMANCE:")
        print("-" * 40)
        
        total_mae = 0
        for i, vital in enumerate(self.vital_signs_labels):
            vital_mae = mean_absolute_error(y_test[:, i], y_pred[:, i])
            total_mae += vital_mae
            
            mean_true = np.mean(y_test[:, i])
            mean_pred = np.mean(y_pred[:, i])
            
            print(f"{vital:<15}: MAE={vital_mae:6.2f} | True Œº={mean_true:6.1f} | Pred Œº={mean_pred:6.1f}")
        
        overall_mae = total_mae / len(self.vital_signs_labels)
        print(f"\nüéØ OVERALL MAE: {overall_mae:.2f}")
        
        # Sample predictions
        print(f"\nüìã SAMPLE PREDICTIONS:")
        print("-" * 60)
        
        for i in range(min(5, len(y_pred))):
            print(f"\nSample {i+1}:")
            print("  Vital          Predicted  Actual    Error")
            print("  " + "-"*40)
            
            for j, vital in enumerate(self.vital_signs_labels):
                pred_val = y_pred[i][j]
                true_val = y_test[i][j]
                error = abs(pred_val - true_val)
                
                if vital == 'temperature':
                    print(f"  {vital:<12}: {pred_val:8.1f}  {true_val:6.1f}  {error:6.1f}")
                else:
                    print(f"  {vital:<12}: {pred_val:8.0f}  {true_val:6.0f}  {error:6.0f}")

def main():
    """Main execution function"""
    
    dataset_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../generated_heart_monitors'))
    
    if not os.path.exists(dataset_dir):
        print(f"‚ùå Dataset directory not found: {dataset_dir}")
        return
    
    # Create trainer
    trainer = PersistentCNNTrainer()
    
    # Train until target is achieved
    success = trainer.train_until_target(
        dataset_dir=dataset_dir,
        target_mae=3.0,  # Realistic target for this dataset size
        max_iterations=30,
        epochs_per_iteration=30
    )
    
    if success:
        print(f"\nüéâ SUCCESS! Target MAE achieved!")
    else:
        print(f"\n‚ö†Ô∏è Target not reached, but best model saved.")
    
    # Save training history
    with open('persistent_training_history.json', 'w') as f:
        json.dump(trainer.training_history, f, indent=2)
    
    print(f"\nüìÑ Training history saved to 'persistent_training_history.json'")

if __name__ == "__main__":
    main()
